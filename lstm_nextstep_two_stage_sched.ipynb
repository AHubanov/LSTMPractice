{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AHubanov/LSTMPractice/blob/main/lstm_nextstep_two_stage_sched.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ca7f6d",
      "metadata": {
        "id": "a4ca7f6d"
      },
      "source": [
        "# LSTM next-step forecasting (train=85%, val=15%, без test split, с Google Drive и проверкой)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "jdzrx-1MIRdr",
      "metadata": {
        "id": "jdzrx-1MIRdr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "80153168",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80153168",
        "outputId": "4c23e7a5-8793-411b-9b8a-14f51b1bdaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DRIVE_DATA_DIR = os.environ.get(\"DRIVE_DATA_DIR\", \"/content/drive/MyDrive/\")\n",
        "DRIVE_OUTPUT_DIR = os.environ.get(\"DRIVE_OUTPUT_DIR\", \"/content/drive/MyDrive/LSTM/artifacts\")\n",
        "os.makedirs(\"datasets\", exist_ok=True)\n",
        "src = os.path.join(DRIVE_DATA_DIR, \"predict/train.parquet\")\n",
        "if os.path.exists(src):\n",
        "    shutil.copy(src, \"datasets/train.parquet\")\n",
        "PARQUET_PATH = \"datasets/train.parquet\" if os.path.exists(\"datasets/train.parquet\") else \"train.parquet\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "ChVW1nJZB15q",
        "outputId": "f46e2aa9-7e3a-4ff7-88a3-200030371d5a"
      },
      "id": "ChVW1nJZB15q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "y0WAzyIQLEjb",
      "metadata": {
        "id": "y0WAzyIQLEjb"
      },
      "outputs": [],
      "source": [
        "WINDOW = 100\n",
        "BATCH_SIZE = 192\n",
        "EPOCHS = 200\n",
        "LR = 0.8e-3\n",
        "MIN_LR = 0.0001\n",
        "HIDDEN_SIZE = 192\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.4\n",
        "PATIENCE = 20\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Bq8haw4H9bXq",
      "metadata": {
        "id": "Bq8haw4H9bXq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(PARQUET_PATH)\n",
        "required_cols = {\"seq_ix\", \"step_in_seq\", \"need_prediction\"}\n",
        "missing = required_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Нет колонок: {missing}\")\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in [\"seq_ix\", \"step_in_seq\", \"need_prediction\"]]\n",
        "feature_cols = sorted(feature_cols, key=lambda x: int(x))\n",
        "\n",
        "df = df.sort_values([\"seq_ix\", \"step_in_seq\"]).reset_index(drop=True)\n",
        "all_seq = df[\"seq_ix\"].unique()\n",
        "rng = np.random.default_rng(SEED)\n",
        "rng.shuffle(all_seq)\n",
        "n_total = len(all_seq)\n",
        "n_train = int(0.85 * n_total)\n",
        "train_seq_ids = all_seq[:n_train]\n",
        "val_seq_ids   = all_seq[n_train:]\n",
        "\n",
        "df[\"_pos\"] = df.groupby(\"seq_ix\").cumcount()\n",
        "df[\"_len\"] = df.groupby(\"seq_ix\")[\"_pos\"].transform(\"max\") + 1\n",
        "df[\"_split\"] = (0.85 * df[\"_len\"]).astype(int)\n",
        "df[\"_val_start\"] = (df[\"_split\"] - WINDOW).clip(lower=0)\n",
        "\n",
        "train_mask = df[\"_pos\"] < df[\"_split\"]\n",
        "val_mask = df[\"_pos\"] >= df[\"_val_start\"]\n",
        "\n",
        "train_seq = df.loc[train_mask].drop(columns=[\"_pos\", \"_len\", \"_split\", \"_val_start\"]).reset_index(drop=True)\n",
        "\n",
        "val_seq = df.loc[val_mask].copy()\n",
        "val_seq[\"need_prediction\"] = val_seq[\"_pos\"] >= val_seq[\"_split\"]\n",
        "val_seq = val_seq.drop(columns=[\"_pos\", \"_len\", \"_split\", \"_val_start\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3c961f",
      "metadata": {
        "id": "bd3c961f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# df = pd.read_parquet(PARQUET_PATH)\n",
        "# required_cols = {\"seq_ix\", \"step_in_seq\", \"need_prediction\"}\n",
        "# missing = required_cols - set(df.columns)\n",
        "# if missing:\n",
        "#     raise ValueError(f\"Нет колонок: {missing}\")\n",
        "# feature_cols = [c for c in df.columns if c not in [\"seq_ix\", \"step_in_seq\", \"need_prediction\"]]\n",
        "# feature_cols = sorted(feature_cols, key=lambda x: int(x))\n",
        "# df = df.sort_values([\"seq_ix\", \"step_in_seq\"]).reset_index(drop=True)\n",
        "# all_seq = df[\"seq_ix\"].unique()\n",
        "# rng = np.random.default_rng(SEED)\n",
        "# rng.shuffle(all_seq)\n",
        "# n_total = len(all_seq)\n",
        "# n_train = int(0.85 * n_total)\n",
        "# train_seq = all_seq[:n_train]\n",
        "# val_seq   = all_seq[n_train:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ebdbcf02",
      "metadata": {
        "id": "ebdbcf02"
      },
      "outputs": [],
      "source": [
        "\n",
        "def subset_by_seq(d: pd.DataFrame, seq_ids):\n",
        "    return d[d[\"seq_ix\"].isin(seq_ids)].copy()\n",
        "\n",
        "df_train_raw = subset_by_seq(df, train_seq)\n",
        "df_val_raw   = subset_by_seq(df, val_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f049d27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f049d27",
        "outputId": "b9841d96-c461-4945-8f3b-709d246c6d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows: train=387750, val=77550\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "df_train_raw = train_seq.copy()\n",
        "df_val_raw   = val_seq.copy()\n",
        "\n",
        "def build_windows(df_seq: pd.DataFrame, window: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    feats = df_seq[feature_cols].values\n",
        "    steps = df_seq[\"step_in_seq\"].values\n",
        "    out = []\n",
        "    n = len(df_seq)\n",
        "    for i in range(window, n):\n",
        "        window_steps = steps[i-window:i+1]\n",
        "        if not np.all(np.diff(window_steps) == 1):\n",
        "            continue\n",
        "        X = feats[i-window:i]\n",
        "        y = feats[i]\n",
        "        out.append((X, y))\n",
        "    return out\n",
        "\n",
        "def build_all_windows(df_raw: pd.DataFrame, window: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    data = []\n",
        "    for _, g in df_raw.groupby(\"seq_ix\"):\n",
        "        g = g.sort_values(\"step_in_seq\")\n",
        "        data.extend(build_windows(g, window))\n",
        "    return data\n",
        "\n",
        "train_pairs = build_all_windows(df_train_raw, WINDOW)\n",
        "val_pairs   = build_all_windows(df_val_raw, WINDOW)\n",
        "\n",
        "print(f\"Windows: train={len(train_pairs)}, val={len(val_pairs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "33da4797",
      "metadata": {
        "id": "33da4797"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_stack = np.concatenate([p[0] for p in train_pairs], axis=0)\n",
        "y_train_stack = np.stack([p[1] for p in train_pairs], axis=0)\n",
        "all_train_for_scaler = np.vstack([X_train_stack, y_train_stack])\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(all_train_for_scaler)\n",
        "\n",
        "def scale_pairs(pairs):\n",
        "    Xs = []\n",
        "    ys = []\n",
        "    for X, y in pairs:\n",
        "        Xs.append(scaler.transform(X))\n",
        "        ys.append(scaler.transform(y.reshape(1, -1)).squeeze(0))\n",
        "    return Xs, ys\n",
        "\n",
        "X_train, y_train = scale_pairs(train_pairs)\n",
        "X_val,   y_val   = scale_pairs(val_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0b8bde83",
      "metadata": {
        "id": "0b8bde83"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X_list, y_list):\n",
        "        self.X = [torch.tensor(x, dtype=torch.float32) for x in X_list]\n",
        "        self.y = [torch.tensor(y, dtype=torch.float32) for y in y_list]\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = WindowDataset(X_train, y_train)\n",
        "val_ds   = WindowDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "input_size = len(feature_cols)\n",
        "output_size = len(feature_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "de2d08f0",
      "metadata": {
        "id": "de2d08f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=256, num_layers=2, dropout=0.18, output_size=31):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers>1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        d = hidden_size\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(d),\n",
        "            nn.Linear(d, d*2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d*2, output_size)\n",
        "        )\n",
        "        for name, p in self.lstm.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                nn.init.orthogonal_(p)\n",
        "            elif \"weight_ih\" in name:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "            elif \"bias\" in name:\n",
        "                nn.init.zeros_(p)\n",
        "        for m in self.head.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        o, _ = self.lstm(x)\n",
        "        h = o[:, -1, :]\n",
        "        return self.head(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "182591b4",
      "metadata": {
        "id": "182591b4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def _weighted_mse(y_pred, y_true):\n",
        "    d = y_pred.shape[-1]\n",
        "    w = torch.ones(d, device=y_pred.device)\n",
        "    k = 5 if d >= 5 else d\n",
        "    if k > 0:\n",
        "        w[:k] = 1.5\n",
        "    while w.dim() < y_pred.dim():\n",
        "        w = w.unsqueeze(0)\n",
        "    e = (y_pred - y_true) ** 2\n",
        "    return (e * w).mean()\n",
        "\n",
        "model = LSTMModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    output_size=output_size\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "criterion = _weighted_mse\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "no_improve = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c576cd48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "c576cd48",
        "outputId": "7f1bcca9-3286-4288-ddf8-f552a3e74eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   0%|          | 1/200 [02:16<7:33:19, 136.68s/it, lr=0.000800, train_MSE=0.732642, val_MSE=0.693151]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   1%|          | 2/200 [04:32<7:29:36, 136.25s/it, lr=0.000800, train_MSE=0.692483, val_MSE=0.683246]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   2%|▏         | 3/200 [06:48<7:26:51, 136.10s/it, lr=0.000800, train_MSE=0.683118, val_MSE=0.679133]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   2%|▏         | 4/200 [09:04<7:24:20, 136.03s/it, lr=0.000800, train_MSE=0.677337, val_MSE=0.676086]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   2%|▎         | 5/200 [11:20<7:22:08, 136.04s/it, lr=0.000800, train_MSE=0.672701, val_MSE=0.674832]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   3%|▎         | 6/200 [13:36<7:19:44, 136.00s/it, lr=0.000800, train_MSE=0.669609, val_MSE=0.671433]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   4%|▎         | 7/200 [15:52<7:17:20, 135.96s/it, lr=0.000800, train_MSE=0.666209, val_MSE=0.669610]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   4%|▍         | 8/200 [18:08<7:15:00, 135.94s/it, lr=0.000800, train_MSE=0.663111, val_MSE=0.669368]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   4%|▍         | 9/200 [20:24<7:12:35, 135.89s/it, lr=0.000800, train_MSE=0.660017, val_MSE=0.667733]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   6%|▌         | 12/200 [27:11<7:05:41, 135.86s/it, lr=0.000680, train_MSE=0.654983, val_MSE=0.667307]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:   8%|▊         | 17/200 [38:30<6:54:24, 135.87s/it, lr=0.000491, train_MSE=0.647862, val_MSE=0.665909]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: 011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP:  14%|█▍        | 28/200 [1:03:38<6:30:56, 136.37s/it, lr=0.000218, train_MSE=0.639083, val_MSE=0.665988]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2098930136.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os, re, pickle, copy, time, torch, torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_PATH = \"/content/artifacts/common.pt\"\n",
        "SCALER_PATH = \"/content/artifacts/common.pkl\"\n",
        "CKPT_PATH = \"/content/artifacts/best.ckpt\"\n",
        "\n",
        "def _stem_and_dir(p):\n",
        "    d = os.path.dirname(p)\n",
        "    b = os.path.basename(p)\n",
        "    stem, ext = os.path.splitext(b)\n",
        "    return d if d else \".\", stem\n",
        "\n",
        "ART_DIR, MODEL_STEM = _stem_and_dir(MODEL_PATH)\n",
        "_, SCALER_STEM = _stem_and_dir(SCALER_PATH)\n",
        "_, CKPT_STEM = _stem_and_dir(CKPT_PATH)\n",
        "\n",
        "def _next_index(d, stem):\n",
        "    if not os.path.exists(d):\n",
        "        return 1\n",
        "    mx = 0\n",
        "    for f in os.listdir(d):\n",
        "        m = re.fullmatch(rf\"{re.escape(stem)}_(\\d{{3}})\\.(pt|pkl|ckpt)\", f)\n",
        "        if m:\n",
        "            mx = max(mx, int(m.group(1)))\n",
        "    return mx + 1\n",
        "\n",
        "def latest_ckpt(d, stem, fallback):\n",
        "    best = None\n",
        "    bi = -1\n",
        "    if os.path.exists(d):\n",
        "        for f in os.listdir(d):\n",
        "            m = re.fullmatch(rf\"{re.escape(stem)}_(\\d{{3}})\\.ckpt\", f)\n",
        "            if m:\n",
        "                i = int(m.group(1))\n",
        "                if i > bi:\n",
        "                    bi = i\n",
        "                    best = os.path.join(d, f)\n",
        "    if best is None and os.path.exists(fallback):\n",
        "        return fallback\n",
        "    return best\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "best_optim_state = None\n",
        "best_sched_state = None\n",
        "no_improve = 0\n",
        "\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.85, patience=1)\n",
        "prev_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "epoch_bar = tqdm(range(1, EPOCHS + 1), desc=\"EP\", position=0, leave=True)\n",
        "for epoch in epoch_bar:\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        yhat = model(xb)\n",
        "        loss = criterion(yhat, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(train_ds)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            yhat = model(xb)\n",
        "            loss = criterion(yhat, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(val_ds) if len(val_ds) > 0 else 1\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    if current_lr < prev_lr:\n",
        "        ckpt_path = latest_ckpt(ART_DIR, CKPT_STEM, CKPT_PATH)\n",
        "        if ckpt_path is not None and os.path.exists(ckpt_path):\n",
        "            ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "            if \"model\" in ckpt:\n",
        "                model.load_state_dict(ckpt[\"model\"])\n",
        "            if \"optimizer\" in ckpt and ckpt[\"optimizer\"] is not None:\n",
        "                optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "            for g in optimizer.param_groups:\n",
        "                g[\"lr\"] = current_lr\n",
        "            if \"scheduler\" in ckpt and ckpt[\"scheduler\"] is not None:\n",
        "                try:\n",
        "                    scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "                except Exception:\n",
        "                    pass\n",
        "            no_improve = 0\n",
        "        prev_lr = current_lr\n",
        "\n",
        "    epoch_bar.set_postfix(train_MSE=f\"{train_loss:.6f}\", val_MSE=f\"{val_loss:.6f}\", lr=f\"{current_lr:.6f}\")\n",
        "\n",
        "    if val_loss + 1e-9 < best_val:\n",
        "        best_val = val_loss\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        idx = _next_index(ART_DIR, MODEL_STEM)\n",
        "        model_path_i = os.path.join(ART_DIR, f\"{MODEL_STEM}_{idx:03d}.pt\")\n",
        "        scaler_path_i = os.path.join(ART_DIR, f\"{SCALER_STEM}_{idx:03d}.pkl\")\n",
        "        ckpt_path_i = os.path.join(ART_DIR, f\"{CKPT_STEM}_{idx:03d}.ckpt\")\n",
        "\n",
        "        torch.save(best_state, model_path_i)\n",
        "        try:\n",
        "            with open(scaler_path_i, \"wb\") as f:\n",
        "                pickle.dump(scaler, f)\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "        prev_meta = {}\n",
        "        prev_ckpt_path = latest_ckpt(ART_DIR, CKPT_STEM, CKPT_PATH)\n",
        "        if prev_ckpt_path is not None and os.path.exists(prev_ckpt_path):\n",
        "            try:\n",
        "                prev_meta = torch.load(prev_ckpt_path, map_location=\"cpu\")\n",
        "            except Exception:\n",
        "                prev_meta = {}\n",
        "\n",
        "        ckpt = dict(prev_meta)\n",
        "        ckpt.update({\n",
        "            \"model\": best_state,\n",
        "            \"optimizer\": copy.deepcopy(optimizer.state_dict()),\n",
        "            \"scheduler\": copy.deepcopy(scheduler.state_dict()),\n",
        "            \"best_val\": best_val,\n",
        "            \"epoch\": epoch,\n",
        "        })\n",
        "        if hasattr(model, \"hidden_size\"):\n",
        "            ckpt[\"hidden_size\"] = model.hidden_size\n",
        "        if hasattr(model, \"config\"):\n",
        "            ckpt[\"config\"] = model.config\n",
        "\n",
        "        torch.save(ckpt, ckpt_path_i)\n",
        "        print(f\"✅ Saved: {idx:03d}\")\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE or current_lr <= MIN_LR:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "00f193ca",
      "metadata": {
        "id": "00f193ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49e5fb3-0e74-4838-89fc-aa533d6037c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сохранено: artifacts/lstm_nextstep.pt, artifacts/standard_scaler.pkl\n",
            "Скопировано в Google Drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "torch.save({\n",
        "    \"state_dict\": model.state_dict(),\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"num_layers\": NUM_LAYERS,\n",
        "    \"dropout\": DROPOUT,\n",
        "    \"output_size\": output_size,\n",
        "    \"window\": WINDOW,\n",
        "    \"feature_cols\": feature_cols\n",
        "}, \"artifacts/lstm_nextstep.pt\")\n",
        "joblib.dump(scaler, \"artifacts/standard_scaler.pkl\")\n",
        "print(\"Сохранено: artifacts/lstm_nextstep.pt, artifacts/standard_scaler.pkl\")\n",
        "try:\n",
        "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "    shutil.copy(\"artifacts/lstm_nextstep.pt\", os.path.join(DRIVE_OUTPUT_DIR, \"lstm_nextstep.pt\"))\n",
        "    shutil.copy(\"artifacts/standard_scaler.pkl\", os.path.join(DRIVE_OUTPUT_DIR, \"standard_scaler.pkl\"))\n",
        "    print(\"Скопировано в Google Drive\")\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-aCHN65PT6_V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "841e61ae18fd42a88121c6fdaab05f52",
            "89a4b29cf8fb426baf04821386371ced",
            "842f7e62e84a4d88ba5ba411e5e277a7",
            "f5db61991bc249129ca9ac721e23588b",
            "d812ac02394845b1b94c4680ecc5a97d",
            "73cb36c67ee04b379dd6efc6e3796164",
            "6ce2dbcc5f634abeb86a760acdc15817",
            "835f941c493042e191328bf5675866a7",
            "ce9ce54dbccd4fbf8ccc303b77d005b7",
            "63fa9c6268be4aa482154b7b82ed1839",
            "3f35638c04ed4d32abb4141ee97fed90"
          ]
        },
        "id": "-aCHN65PT6_V",
        "outputId": "319549b2-dd41-48ac-bc5b-6362577b39a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, window=100, rows=517000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/517000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "841e61ae18fd42a88121c6fdaab05f52"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "from sklearn.metrics import r2_score\n",
        "import torch\n",
        "from torch import nn\n",
        "import joblib\n",
        "\n",
        "PARQUET_PATH = \"/content/datasets/train.parquet\"\n",
        "MODEL_PATH = \"/content/artifacts/lstm_nextstep.pt\"\n",
        "SCALER_PATH = \"/content/artifacts/standard_scaler.pkl\"\n",
        "FRAC_SEQ = 1\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@dataclass\n",
        "class DataPoint:\n",
        "    seq_ix: int\n",
        "    step_in_seq: int\n",
        "    need_prediction: bool\n",
        "    state: np.ndarray\n",
        "\n",
        "class ScorerStepByStep:\n",
        "    def __init__(self, dataset_path: str, frac_seq: float, seed: int, min_len: int):\n",
        "        df = pd.read_parquet(dataset_path).sort_values([\"seq_ix\",\"step_in_seq\"])\n",
        "        groups = [g for _, g in df.groupby(\"seq_ix\", sort=False) if len(g) >= min_len]\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n = len(groups)\n",
        "        k = max(1, int(round(n * frac_seq)))\n",
        "        idx = rng.choice(n, size=k, replace=False)\n",
        "        subset = pd.concat([groups[i] for i in sorted(idx)], axis=0).reset_index(drop=True)\n",
        "        self.dataset = subset\n",
        "        self.dim = self.dataset.shape[1] - 3\n",
        "        self.features = self.dataset.columns[3:]\n",
        "    def score(self, model) -> dict:\n",
        "        predictions, targets = [], []\n",
        "        next_prediction = None\n",
        "        for row in tqdm(self.dataset.values):\n",
        "            seq_ix, step_in_seq, need_prediction = row[:3]\n",
        "            new_state = row[3:]\n",
        "            if next_prediction is not None:\n",
        "                predictions.append(next_prediction)\n",
        "                targets.append(new_state)\n",
        "            dp = DataPoint(seq_ix, step_in_seq, need_prediction, new_state)\n",
        "            next_prediction = model.predict(dp)\n",
        "            if need_prediction and next_prediction is None:\n",
        "                raise RuntimeError(\"predict returned None when need_prediction==1\")\n",
        "        if len(predictions) == 0:\n",
        "            return {\"mean_r2\": np.nan}\n",
        "        P = np.asarray(predictions, dtype=np.float32)\n",
        "        T = np.asarray(targets, dtype=np.float32)\n",
        "        scores = {f: r2_score(T[:, i], P[:, i]) for i, f in enumerate(self.features)}\n",
        "        scores[\"mean_r2\"] = float(np.nanmean(list(scores.values())))\n",
        "        return scores\n",
        "\n",
        "def _infer_num_layers_from_state_dict(sd):\n",
        "    n = 0\n",
        "    while f\"lstm.weight_ih_l{n}\" in sd:\n",
        "        n += 1\n",
        "    return max(n, 1)\n",
        "\n",
        "def _infer_hidden_input_from_state_dict(sd):\n",
        "    w_ih = sd.get(\"lstm.weight_ih_l0\", None)\n",
        "    if w_ih is None:\n",
        "        return None, None\n",
        "    hidden_size = w_ih.shape[0] // 4\n",
        "    input_size = w_ih.shape[1]\n",
        "    return hidden_size, input_size\n",
        "\n",
        "class PredictionModel:\n",
        "    def __init__(self, ckpt_path: str, scaler_path: str, device: str):\n",
        "        self.device = device\n",
        "        raw = torch.load(ckpt_path, map_location=device)\n",
        "        if isinstance(raw, dict) and \"state_dict\" in raw:\n",
        "            meta = raw\n",
        "            state_dict = raw[\"state_dict\"]\n",
        "        elif isinstance(raw, dict):\n",
        "            meta = {}\n",
        "            state_dict = raw\n",
        "        else:\n",
        "            meta = {}\n",
        "            state_dict = raw\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        if hasattr(self.scaler, \"n_features_in_\"):\n",
        "            nfeat = int(self.scaler.n_features_in_)\n",
        "        elif hasattr(self.scaler, \"mean_\"):\n",
        "            nfeat = int(self.scaler.mean_.shape[0])\n",
        "        else:\n",
        "            nfeat = None\n",
        "        hidden_sd, input_sd = _infer_hidden_input_from_state_dict(state_dict)\n",
        "        num_layers_sd = _infer_num_layers_from_state_dict(state_dict)\n",
        "        input_size = int(meta.get(\"input_size\", input_sd if input_sd is not None else nfeat))\n",
        "        output_size = int(meta.get(\"output_size\", nfeat))\n",
        "        hidden_size = int(meta.get(\"hidden_size\", hidden_sd if hidden_sd is not None else 256))\n",
        "        num_layers = int(meta.get(\"num_layers\", num_layers_sd))\n",
        "        dropout = float(meta.get(\"dropout\", 0.18))\n",
        "        self.window = int(meta.get(\"window\", 50))\n",
        "        self.model = LSTMModel(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            output_size=output_size\n",
        "        ).to(device)\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "        self.model.eval()\n",
        "        self.current_seq_ix = None\n",
        "        self.sequence_history: List[np.ndarray] = []\n",
        "        self.maxlen = self.window\n",
        "        self.expected_dim = output_size\n",
        "    def _reset_if_new_sequence(self, seq_ix: int):\n",
        "        if self.current_seq_ix != seq_ix:\n",
        "            self.current_seq_ix = seq_ix\n",
        "            self.sequence_history = []\n",
        "    def _append_state(self, state: np.ndarray):\n",
        "        self.sequence_history.append(np.asarray(state, dtype=np.float32))\n",
        "        if len(self.sequence_history) > self.maxlen:\n",
        "            self.sequence_history = self.sequence_history[-self.maxlen:]\n",
        "    @torch.no_grad()\n",
        "    def _predict_from_window(self, window_np: np.ndarray) -> np.ndarray:\n",
        "        window_scaled = self.scaler.transform(window_np)\n",
        "        x = torch.from_numpy(window_scaled[None, ...]).to(self.device).float()\n",
        "        yhat_scaled = self.model(x).detach().cpu().numpy()[0]\n",
        "        yhat = self.scaler.inverse_transform(yhat_scaled.reshape(1, -1))[0]\n",
        "        return yhat.astype(np.float32)\n",
        "    def predict(self, data_point: DataPoint) -> np.ndarray | None:\n",
        "        seq_ix = int(data_point.seq_ix)\n",
        "        need_pred = int(data_point.need_prediction)\n",
        "        self._reset_if_new_sequence(seq_ix)\n",
        "        s = np.asarray(data_point.state, dtype=np.float32)\n",
        "        if s.shape[0] != self.expected_dim:\n",
        "            return None\n",
        "        self._append_state(s)\n",
        "        if need_pred != 1 or len(self.sequence_history) < self.maxlen:\n",
        "            return None\n",
        "        window_np = np.stack(self.sequence_history[-self.maxlen:], axis=0)\n",
        "        return self._predict_from_window(window_np)\n",
        "\n",
        "assert DEVICE==\"cuda\", \"Нужен GPU: включи среду с CUDA (в Colab: Runtime → Change runtime type → GPU).\"\n",
        "model = PredictionModel(MODEL_PATH, SCALER_PATH, DEVICE)\n",
        "scorer = ScorerStepByStep(PARQUET_PATH, frac_seq=FRAC_SEQ, seed=SEED, min_len=model.window+1)\n",
        "print(f\"Device: {DEVICE}, window={model.window}, rows={len(scorer.dataset)}\")\n",
        "res = scorer.score(model)\n",
        "print(f\"Mean R²: {res['mean_r2']:.6f}\")\n",
        "feats = list(scorer.features[:5])\n",
        "for f in feats:\n",
        "    print(f\"{f}: {res[f]:.6f}\")\n",
        "print(f\"Total features: {len(scorer.features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5675b763",
      "metadata": {
        "id": "5675b763"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "try:\n",
        "    import joblib\n",
        "except Exception:\n",
        "    joblib = None\n",
        "\n",
        "MODEL_PATH = \"/content/artifacts/common_009.pt\"\n",
        "SCALER_PATH = \"/content/artifacts/common_009.pkl\"\n",
        "\n",
        "LR = 5e-4\n",
        "PATIENCE = 5\n",
        "\n",
        "model = LSTMModel(\n",
        "    input_size=input_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    output_size=output_size\n",
        ").to(DEVICE)\n",
        "\n",
        "ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "state_dict = ckpt.get(\"state_dict\", ckpt.get(\"model_state_dict\", ckpt))\n",
        "state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
        "state_dict = {k.replace(\"model.\", \"\", 1): v for k, v in state_dict.items()}\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "def load_artifact(path, map_location=None):\n",
        "    if joblib is not None:\n",
        "        try:\n",
        "            return joblib.load(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        return torch.load(path, map_location=map_location)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        with gzip.open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Не удалось загрузить артефакт {path}: {e}\")\n",
        "\n",
        "scaler = load_artifact(SCALER_PATH, map_location=DEVICE)\n",
        "\n",
        "# N_IDX = 1\n",
        "# N_WEIGHT = 5.0\n",
        "\n",
        "# def _weighted_mse(y_pred, y_true):\n",
        "#     d = y_pred.shape[-1]\n",
        "#     w = torch.ones(d, device=y_pred.device)\n",
        "#     if 0 <= N_IDX < d:\n",
        "#         w[N_IDX] = N_WEIGHT\n",
        "#     while w.dim() < y_pred.dim():\n",
        "#         w = w.unsqueeze(0)\n",
        "#     e = (y_pred - y_true) ** 2\n",
        "#     return (e * w).mean()\n",
        "\n",
        "def weighted_mse_adaptive_focus(y_pred, y_true, k=5, max_bias=3.0, eps=1e-8):\n",
        "    e = (y_pred - y_true) ** 2\n",
        "    var = e.mean(dim=0, keepdim=True)\n",
        "    w = (var / (var.mean() + eps)).clamp_min(0.5).clamp_max(2.0)\n",
        "    b = torch.ones_like(w)\n",
        "    if k > 0:\n",
        "        b[..., :k] = max_bias\n",
        "    w = (w * b).clamp_max(3.0)\n",
        "    return (e * w).mean()\n",
        "\n",
        "\n",
        "\n",
        "criterion = weighted_mse_adaptive_focus\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "best_val = float('inf')\n",
        "best_state = None\n",
        "no_improve = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "867fc307",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "867fc307",
        "outputId": "d39f35a5-4229-40ef-ed31-fa90456ba4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model and Training Parameters ===\n",
            "      WINDOW: 100\n",
            "  BATCH_SIZE: 192\n",
            "      EPOCHS: 200\n",
            "          LR: 0.0005\n",
            " HIDDEN_SIZE: 192\n",
            "  NUM_LAYERS: 2\n",
            "     DROPOUT: 0.4\n",
            "    PATIENCE: 5\n",
            "      DEVICE: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:   0%|          | 1/200 [02:16<7:31:08, 136.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | train MSE: 0.799804 | val MSE: 0.847319 | time 136.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   1%|          | 2/200 [04:31<7:28:39, 135.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2 | train MSE: 0.795431 | val MSE: 0.848596 | time 135.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   2%|▏         | 3/200 [06:47<7:26:16, 135.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   3 | train MSE: 0.790971 | val MSE: 0.847486 | time 135.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   2%|▏         | 4/200 [09:03<7:23:52, 135.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   4 | train MSE: 0.782749 | val MSE: 0.851550 | time 135.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   2%|▎         | 5/200 [11:19<7:21:36, 135.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | train MSE: 0.778067 | val MSE: 0.859431 | time 135.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   2%|▎         | 5/200 [13:35<8:49:57, 163.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   6 | train MSE: 0.770740 | val MSE: 0.858098 | time 135.8s\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(32, 192, num_layers=2, batch_first=True, dropout=0.4)\n",
              "  (head): Sequential(\n",
              "    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=192, out_features=384, bias=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Linear(in_features=384, out_features=32, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=1)\n",
        "print(\"=== Model and Training Parameters ===\")\n",
        "for k, v in {\n",
        "    \"WINDOW\": WINDOW,\n",
        "    \"BATCH_SIZE\": BATCH_SIZE,\n",
        "    \"EPOCHS\": EPOCHS,\n",
        "    \"LR\": LR,\n",
        "    \"HIDDEN_SIZE\": HIDDEN_SIZE,\n",
        "    \"NUM_LAYERS\": NUM_LAYERS,\n",
        "    \"DROPOUT\": DROPOUT,\n",
        "    \"PATIENCE\": PATIENCE,\n",
        "    \"DEVICE\": DEVICE,\n",
        "}.items():\n",
        "    print(f\"{k:>12}: {v}\")\n",
        "\n",
        "epoch_bar = tqdm(range(1, EPOCHS + 1), desc=\"Epochs\")\n",
        "for epoch in epoch_bar:\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        yhat = model(xb)\n",
        "        loss = criterion(yhat, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "    train_loss /= len(train_ds)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            yhat = model(xb)\n",
        "            loss = criterion(yhat, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(val_ds) if len(val_ds) > 0 else 1\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Epoch {epoch:3d} | train MSE: {train_loss:.6f} | val MSE: {val_loss:.6f} | time {dt:.1f}s\")\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss + 1e-9 < best_val:\n",
        "        best_val = val_loss\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c65bf2fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c65bf2fa",
        "outputId": "59a551d5-ea56-429c-9efe-760d8c63297e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сохранено: artifacts/lstm_nextstep.pt, artifacts/standard_scaler.pkl\n",
            "Скопировано в Google Drive\n"
          ]
        }
      ],
      "source": [
        "net = getattr(model, \"model\", model)\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "torch.save({\n",
        "    \"state_dict\": net.state_dict(),\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"num_layers\": NUM_LAYERS,\n",
        "    \"dropout\": DROPOUT,\n",
        "    \"output_size\": output_size,\n",
        "    \"window\": WINDOW,\n",
        "    \"feature_cols\": feature_cols\n",
        "}, \"artifacts/lstm_nextstep.pt\")\n",
        "joblib.dump(scaler, \"artifacts/standard_scaler.pkl\")\n",
        "print(\"Сохранено: artifacts/lstm_nextstep.pt, artifacts/standard_scaler.pkl\")\n",
        "try:\n",
        "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "    shutil.copy(\"artifacts/lstm_nextstep.pt\", os.path.join(DRIVE_OUTPUT_DIR, \"lstm_nextstep.pt\"))\n",
        "    shutil.copy(\"artifacts/standard_scaler.pkl\", os.path.join(DRIVE_OUTPUT_DIR, \"standard_scaler.pkl\"))\n",
        "    print(\"Скопировано в Google Drive\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "74982672",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389,
          "referenced_widgets": [
            "d8472518c95a430994b9a2283d1c0cde",
            "cb306fe9c5a34d86a351e0ca27ab2b10",
            "4cd94e54717642ff8ee99a2399f3d643",
            "7c41798233a54970975619c89adc9c8a",
            "f51f68af41f24ec4a4e640b6f3d15bf4",
            "76f1e36bfaa8475ab81367d5c71331d1",
            "2a6209cbe00b4ba08bd1267beea3eb1c",
            "9237ca810b3349b4bc36cf55ce92f608",
            "314f3269dc1a4ca39456382c23fae5f1",
            "675b65222bb04143addd7d89f3bbd583",
            "f38e531f528a42debe4fc358a9ec81d5"
          ]
        },
        "id": "74982672",
        "outputId": "f77a7e98-3a81-49bc-99e9-2a68d42d91c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, window=100, rows=517000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/517000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8472518c95a430994b9a2283d1c0cde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1658712879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScorerStepByStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARQUET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFRAC_SEQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Device: {DEVICE}, window={model.window}, rows={len(scorer.dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean R²: {res['mean_r2']:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1658712879.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_in_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mnext_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mneed_prediction\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnext_prediction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict returned None when need_prediction==1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1658712879.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data_point)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mwindow_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_from_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Нужен GPU: включи среду с CUDA (в Colab: Runtime → Change runtime type → GPU).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1658712879.py\u001b[0m in \u001b[0;36m_predict_from_window\u001b[0;34m(self, window_np)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mwindow_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0myhat_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "from sklearn.metrics import r2_score\n",
        "import torch\n",
        "from torch import nn\n",
        "import joblib\n",
        "\n",
        "PARQUET_PATH = \"/content/datasets/train.parquet\"\n",
        "MODEL_PATH = \"/content/artifacts/lstm_nextstep.pt\"\n",
        "SCALER_PATH = \"/content/artifacts/standard_scaler.pkl\"\n",
        "FRAC_SEQ = 1\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@dataclass\n",
        "class DataPoint:\n",
        "    seq_ix: int\n",
        "    step_in_seq: int\n",
        "    need_prediction: bool\n",
        "    state: np.ndarray\n",
        "\n",
        "class ScorerStepByStep:\n",
        "    def __init__(self, dataset_path: str, frac_seq: float, seed: int, min_len: int):\n",
        "        df = pd.read_parquet(dataset_path).sort_values([\"seq_ix\",\"step_in_seq\"])\n",
        "        groups = [g for _, g in df.groupby(\"seq_ix\", sort=False) if len(g) >= min_len]\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n = len(groups)\n",
        "        k = max(1, int(round(n * frac_seq)))\n",
        "        idx = rng.choice(n, size=k, replace=False)\n",
        "        subset = pd.concat([groups[i] for i in sorted(idx)], axis=0).reset_index(drop=True)\n",
        "        self.dataset = subset\n",
        "        self.dim = self.dataset.shape[1] - 3\n",
        "        self.features = self.dataset.columns[3:]\n",
        "    def score(self, model) -> dict:\n",
        "        predictions, targets = [], []\n",
        "        next_prediction = None\n",
        "        for row in tqdm(self.dataset.values):\n",
        "            seq_ix, step_in_seq, need_prediction = row[:3]\n",
        "            new_state = row[3:]\n",
        "            if next_prediction is not None:\n",
        "                predictions.append(next_prediction)\n",
        "                targets.append(new_state)\n",
        "            dp = DataPoint(seq_ix, step_in_seq, need_prediction, new_state)\n",
        "            next_prediction = model.predict(dp)\n",
        "            if need_prediction and next_prediction is None:\n",
        "                raise RuntimeError(\"predict returned None when need_prediction==1\")\n",
        "        if len(predictions) == 0:\n",
        "            return {\"mean_r2\": np.nan}\n",
        "        P = np.asarray(predictions, dtype=np.float32)\n",
        "        T = np.asarray(targets, dtype=np.float32)\n",
        "        scores = {f: r2_score(T[:, i], P[:, i]) for i, f in enumerate(self.features)}\n",
        "        scores[\"mean_r2\"] = float(np.nanmean(list(scores.values())))\n",
        "        return scores\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=256, num_layers=2, dropout=0.18, output_size=31):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers>1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        d = hidden_size\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(d),\n",
        "            nn.Linear(d, d*2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d*2, output_size)\n",
        "        )\n",
        "        for name, p in self.lstm.named_parameters():\n",
        "            if \"weight_hh\" in name:\n",
        "                nn.init.orthogonal_(p)\n",
        "            elif \"weight_ih\" in name:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "            elif \"bias\" in name:\n",
        "                nn.init.zeros_(p)\n",
        "        for m in self.head.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        o, _ = self.lstm(x)\n",
        "        h = o[:, -1, :]\n",
        "        return self.head(h)\n",
        "\n",
        "def _infer_num_layers_from_state_dict(sd):\n",
        "    n = 0\n",
        "    while f\"lstm.weight_ih_l{n}\" in sd:\n",
        "        n += 1\n",
        "    return max(n, 1)\n",
        "\n",
        "def _infer_hidden_input_from_state_dict(sd):\n",
        "    w_ih = sd.get(\"lstm.weight_ih_l0\", None)\n",
        "    if w_ih is None:\n",
        "        return None, None\n",
        "    hidden_size = w_ih.shape[0] // 4\n",
        "    input_size = w_ih.shape[1]\n",
        "    return hidden_size, input_size\n",
        "\n",
        "class PredictionModel:\n",
        "    def __init__(self, ckpt_path: str, scaler_path: str, device: str):\n",
        "        self.device = device\n",
        "        raw = torch.load(ckpt_path, map_location=device)\n",
        "        if isinstance(raw, dict) and \"state_dict\" in raw:\n",
        "            meta = raw\n",
        "            state_dict = raw[\"state_dict\"]\n",
        "        elif isinstance(raw, dict):\n",
        "            meta = {}\n",
        "            state_dict = raw\n",
        "        else:\n",
        "            meta = {}\n",
        "            state_dict = raw\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        if hasattr(self.scaler, \"n_features_in_\"):\n",
        "            nfeat = int(self.scaler.n_features_in_)\n",
        "        elif hasattr(self.scaler, \"mean_\"):\n",
        "            nfeat = int(self.scaler.mean_.shape[0])\n",
        "        else:\n",
        "            nfeat = None\n",
        "        hidden_sd, input_sd = _infer_hidden_input_from_state_dict(state_dict)\n",
        "        num_layers_sd = _infer_num_layers_from_state_dict(state_dict)\n",
        "        input_size = int(meta.get(\"input_size\", input_sd if input_sd is not None else nfeat))\n",
        "        output_size = int(meta.get(\"output_size\", nfeat))\n",
        "        hidden_size = int(meta.get(\"hidden_size\", hidden_sd if hidden_sd is not None else 256))\n",
        "        num_layers = int(meta.get(\"num_layers\", num_layers_sd))\n",
        "        dropout = float(meta.get(\"dropout\", 0.18))\n",
        "        self.window = int(meta.get(\"window\", 50))\n",
        "        self.model = LSTMModel(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            output_size=output_size\n",
        "        ).to(device)\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "        self.model.eval()\n",
        "        self.current_seq_ix = None\n",
        "        self.sequence_history: List[np.ndarray] = []\n",
        "        self.maxlen = self.window\n",
        "        self.expected_dim = output_size\n",
        "    def _reset_if_new_sequence(self, seq_ix: int):\n",
        "        if self.current_seq_ix != seq_ix:\n",
        "            self.current_seq_ix = seq_ix\n",
        "            self.sequence_history = []\n",
        "    def _append_state(self, state: np.ndarray):\n",
        "        self.sequence_history.append(np.asarray(state, dtype=np.float32))\n",
        "        if len(self.sequence_history) > self.maxlen:\n",
        "            self.sequence_history = self.sequence_history[-self.maxlen:]\n",
        "    @torch.no_grad()\n",
        "    def _predict_from_window(self, window_np: np.ndarray) -> np.ndarray:\n",
        "        window_scaled = self.scaler.transform(window_np)\n",
        "        x = torch.from_numpy(window_scaled[None, ...]).to(self.device).float()\n",
        "        yhat_scaled = self.model(x).detach().cpu().numpy()[0]\n",
        "        yhat = self.scaler.inverse_transform(yhat_scaled.reshape(1, -1))[0]\n",
        "        return yhat.astype(np.float32)\n",
        "    def predict(self, data_point: DataPoint) -> np.ndarray | None:\n",
        "        seq_ix = int(data_point.seq_ix)\n",
        "        need_pred = int(data_point.need_prediction)\n",
        "        self._reset_if_new_sequence(seq_ix)\n",
        "        s = np.asarray(data_point.state, dtype=np.float32)\n",
        "        if s.shape[0] != self.expected_dim:\n",
        "            return None\n",
        "        self._append_state(s)\n",
        "        if need_pred != 1 or len(self.sequence_history) < self.maxlen:\n",
        "            return None\n",
        "        window_np = np.stack(self.sequence_history[-self.maxlen:], axis=0)\n",
        "        return self._predict_from_window(window_np)\n",
        "\n",
        "assert DEVICE==\"cuda\", \"Нужен GPU: включи среду с CUDA (в Colab: Runtime → Change runtime type → GPU).\"\n",
        "model = PredictionModel(MODEL_PATH, SCALER_PATH, DEVICE)\n",
        "scorer = ScorerStepByStep(PARQUET_PATH, frac_seq=FRAC_SEQ, seed=SEED, min_len=model.window+1)\n",
        "print(f\"Device: {DEVICE}, window={model.window}, rows={len(scorer.dataset)}\")\n",
        "res = scorer.score(model)\n",
        "print(f\"Mean R²: {res['mean_r2']:.6f}\")\n",
        "feats = list(scorer.features[:5])\n",
        "for f in feats:\n",
        "    print(f\"{f}: {res[f]:.6f}\")\n",
        "print(f\"Total features: {len(scorer.features)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "841e61ae18fd42a88121c6fdaab05f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89a4b29cf8fb426baf04821386371ced",
              "IPY_MODEL_842f7e62e84a4d88ba5ba411e5e277a7",
              "IPY_MODEL_f5db61991bc249129ca9ac721e23588b"
            ],
            "layout": "IPY_MODEL_d812ac02394845b1b94c4680ecc5a97d"
          }
        },
        "89a4b29cf8fb426baf04821386371ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73cb36c67ee04b379dd6efc6e3796164",
            "placeholder": "​",
            "style": "IPY_MODEL_6ce2dbcc5f634abeb86a760acdc15817",
            "value": "  1%"
          }
        },
        "842f7e62e84a4d88ba5ba411e5e277a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_835f941c493042e191328bf5675866a7",
            "max": 517000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce9ce54dbccd4fbf8ccc303b77d005b7",
            "value": 2636
          }
        },
        "f5db61991bc249129ca9ac721e23588b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63fa9c6268be4aa482154b7b82ed1839",
            "placeholder": "​",
            "style": "IPY_MODEL_3f35638c04ed4d32abb4141ee97fed90",
            "value": " 2636/517000 [00:06&lt;24:44, 346.58it/s]"
          }
        },
        "d812ac02394845b1b94c4680ecc5a97d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cb36c67ee04b379dd6efc6e3796164": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce2dbcc5f634abeb86a760acdc15817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "835f941c493042e191328bf5675866a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce9ce54dbccd4fbf8ccc303b77d005b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63fa9c6268be4aa482154b7b82ed1839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f35638c04ed4d32abb4141ee97fed90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8472518c95a430994b9a2283d1c0cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb306fe9c5a34d86a351e0ca27ab2b10",
              "IPY_MODEL_4cd94e54717642ff8ee99a2399f3d643",
              "IPY_MODEL_7c41798233a54970975619c89adc9c8a"
            ],
            "layout": "IPY_MODEL_f51f68af41f24ec4a4e640b6f3d15bf4"
          }
        },
        "cb306fe9c5a34d86a351e0ca27ab2b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f1e36bfaa8475ab81367d5c71331d1",
            "placeholder": "​",
            "style": "IPY_MODEL_2a6209cbe00b4ba08bd1267beea3eb1c",
            "value": "  0%"
          }
        },
        "4cd94e54717642ff8ee99a2399f3d643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9237ca810b3349b4bc36cf55ce92f608",
            "max": 517000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_314f3269dc1a4ca39456382c23fae5f1",
            "value": 2346
          }
        },
        "7c41798233a54970975619c89adc9c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675b65222bb04143addd7d89f3bbd583",
            "placeholder": "​",
            "style": "IPY_MODEL_f38e531f528a42debe4fc358a9ec81d5",
            "value": " 2346/517000 [00:06&lt;21:36, 397.07it/s]"
          }
        },
        "f51f68af41f24ec4a4e640b6f3d15bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f1e36bfaa8475ab81367d5c71331d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a6209cbe00b4ba08bd1267beea3eb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9237ca810b3349b4bc36cf55ce92f608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314f3269dc1a4ca39456382c23fae5f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "675b65222bb04143addd7d89f3bbd583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f38e531f528a42debe4fc358a9ec81d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}